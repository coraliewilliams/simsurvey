---
title: "Current Practices of Simulation Studies for Statistical Method Evaluation in Ecology and Evolutionary Biology"
#author: "Coralie Williams, Yefeng Yang, Malgorzata Lagisz, Kyle Morrison, Lorenzo Ricolfi, David Warton, and Shinichi Nakagawa"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    toc: true
    toc-location: left
    toc-depth: 2
    theme: sandstone
    embed-resources: true
    code-fold: true
    code-tools: true
    number-sections: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| output: false
#| warning: false
#| label: packages
#| code-overflow: wrap
#| code-fold: true

pacman::p_load(
  rmarkdown, readr, dplyr, purrr, splitstackshape, tidyverse, tidyr, ggplot2,
  vroom, details, sessioninfo, devtools, readxl, forcats, stringr, RColorBrewer,
  patchwork, kableExtra
)

options(digits = 3, scipen = 5)
```

# Background

Simulation studies hold a central role in evaluating statistical methodologies. Conducting a simulation study is similar to an experiment. In a simulation study the underlying process of generating the data is known, and the statistical performance can be assessed under diverse scenarios. Here, we present the findings from a survey of 96 research articles in ecology and evolutionary biology journals that use at least one simulation study to evaluate statistical methods.

A framework called ADEMP (Aims, Data-generating mechanisms, Methods, Estimand/statistical target, Performance measures) was introduced by Morris et al. (2019, Stat Med, 38, p2074), and provides key steps to plan, code, analyse, and report simulation studies.

Using the ADEMP framework as a basis, we put together a list of reporting items to characterise and assess the reporting practices of simulation studies. Our results provide an overview of the current state of simulation studies in ecology and evolutionary biology.

# Objectives

Using the ADEMP+ framework our objective are:

1)   Survey the characteristics of simulation studies evaluating statistical methodologies in ecology and evolutionary biology.

2)   Assess the reporting practices of these simulation studies evaluating statistical methodologies in ecology and evolutionary biology.

# Literature search

## Journal list

```{r journalList, warning=F, message=F}
# Load journal lists
jcr_stats <- read_csv("~/Projects/simsurvey/data/JCR/JCR_JournalResults_09_2022_Statistics_and_Probability.csv", skip = 2)
jcr_eco <- read_csv("~/Projects/simsurvey/data/JCR/JCR_JournalResults_09_2022_Ecology.csv", skip = 2)
jcr_evo <- read_csv("~/Projects/simsurvey/data/JCR/JCR_JournalResults_09_2022_Evolutionary_Biology.csv", skip = 2)

# Clean: delete empty rows and comment lines at the end of files
jcr_stats <- jcr_stats[which(!is.na(jcr_stats$ISSN)),]
jcr_eco <- jcr_eco[which(!is.na(jcr_eco$ISSN)),]
jcr_evo <- jcr_evo[which(!is.na(jcr_evo$ISSN)),]


# Check how many journals overlap between each journal group: none with stats journal list, and 18 journals between eco and evo
stats_in_eco <- jcr_stats[which(jcr_stats$`Journal name` %in% jcr_eco$`Journal name`),]
stats_in_evo <- jcr_stats[which(jcr_stats$`Journal name` %in% jcr_evo$`Journal name`),]
eco_in_evo <- jcr_eco$`Journal name`[which(jcr_eco$`Journal name` %in% jcr_evo$`Journal name`)]
evo_in_eco <- jcr_evo$`Journal name`[which(jcr_evo$`Journal name` %in% jcr_eco$`Journal name`)]

# get concatenated list of eco + evo journal lists
ecoevo <- rbind(jcr_eco, jcr_evo)

## change in code below on 20/09/2023, before this code was merging on all columns and only matched 15 journals and missed 3 journals that were overlapping due to different values in "JIF Quartile" column

# get list of matching and unique journals between eco and evo groups
ecoevo_match <- merge(jcr_eco[,-c(5,8)], jcr_evo[,-c(5,8)], by=c("Journal name","JCR Abbreviation", "ISSN", "eISSN", "Total Citations", "2021 JIF", "2021 JCI", "% of OA Gold"))
ecoevo_match$Category <- "Both"
ecoevo_match$`JIF Quartile` <- "NA"
ecoevo_notmatch <- ecoevo[which(!ecoevo$`Journal name` %in% ecoevo_match$`Journal name`),]


# get merged list of journals from eco and evo
ecoevo_merged <- rbind(ecoevo_match, ecoevo_notmatch)

# save results
write_csv(ecoevo_merged, "~/Projects/simsurvey/data/ecoevo_jcr_journal_list.csv", na="")

```



## Search strings

The custom search string was developed on 13/10/2022 on Web of Science

::: {.callout-tip appearance="minimal" icon="false"}
**(TS=((simulation\*) AND ("method") AND ("compar" OR "evaluat" OR "validat" OR "statist") AND (bias OR coverag\* OR power OR convergence OR"standard error" OR "confidence interval" OR "performance measure" OR "mean error" OR "type I error"))) AND WC=("Ecology" OR "Evolutionary Biology"**
:::

## Inclusion and exclusion criteria

-   Include publications between 01-01-2012 and 01-09-2022.
-   Include studies using simulation for the evaluation of statistical methodologies. Simulation studies that are not assessing methods in statistics will be excluded (for example simulation studies to assess ecological theories: <https://doi.org/10.1016/j.ecolmodel.2019.02.007>).
-   Simulation studies that are part of a review, commentary, letter to the editor or tutorial will be excluded (for example: <https://doi.org/10.1111/2041-210X.1380>) Included studies published in one of predefined journals in Table 1.
-   Include if: Full text is available and full text is in English

::: column-margin
Note, exclusion term criteria were added post-hoc.
:::

### Deviations and additions

Five articles were added post-hoc to the search and screening to reach 100 articles. 

```{r}
# 10/11/2023
# import rayyan decision of 50 papers screening
rayyan_50papers <- read.csv("~/Projects/simsurvey/data/survey/rayyan_abstract_results_50papers.csv")

# randomly sample 5 papers from included list
set.seed(48)
extra_5papers <- rayyan_50papers %>% 
  filter(grepl("Included", notes)) %>% 
  sample_n(5)

# Pseudo-randomly sample one more paper as one paper from the 5 randomly sampled did not meet inclusion criteria in full-text (Wu et al., 2019: is not a simulation study)
set.seed(48)
extra_6papers <- rayyan_50papers %>% 
  filter(grepl("Included", notes)) %>% 
  sample_n(6)

```



# Analysis

```{r}
# Load text extraction
survey_raw <- read_excel("~/Projects/simsurvey/data/survey/simulationSurvey_extraction_100papers_23012024.xlsx", na=c("", "NA", NA))
# remove questions that are comments
survey <- survey_raw %>% select(!matches("comment"))

```


```{r}

# Store 12 reporting questions -------------------
r <- c("1.1.aims_reporting", 
       "2.1.datagen_reporting", 
       "3.1.estimandtarget_reporting",
       "4.1.method_reporting",
       "5.1.perfmeasures_reporting",
       "5.3.perfmeasures_lessknown",
       "6.1.software_reporting",
       "6.4.software_Rpackage",
       "7.1.code_datagen",
       "7.2.code_datagen_allsteps",
       "8.1.code_perfmeasures",
       "10.1.analysis_perfmeasures_reporting")

# only planning stage 
r1 <- c("1.1.aims_reporting", 
       "2.1.datagen_reporting", 
       "3.1.estimandtarget_reporting",
       "4.1.method_reporting",
       "5.1.perfmeasures_reporting")

# only coding stage
r2 <- c("6.1.software_reporting")

# only analysis stage
r3 <- c("9.1.analysis_exploratory",
       "10.1.analysis_perfmeasures_reporting",
       "11.1.analysis_montecarlo_uncertainty")


# Store 12 reporting-section questions ------------
rs <- c("1.2.aims_reportingsection", 
        "2.2.datagen_reportingsection", 
        "2.5.2.datagen_parameters_section",
        "2.6.2.datagen_conditions_section",
        "2.7.2.datagen_simulationruns_section",
        "3.2.estimandtarget_reportingsection",
        "4.2.method_reportingsection",
        "5.2.perfmeasures_reportingsection",
        "6.2.software_reportingsection",
        "9.2.analysis_exploratory_reporting",
        "10.2.analysis_perfmeasures_reportingsection",
        "11.2.analysis_montecarlo_reportingsection")

# only planning stage
rs1 <- c("1.2.aims_reportingsection", 
        "2.2.datagen_reportingsection", 
        "3.2.estimandtarget_reportingsection",
        "4.2.method_reportingsection",
        "5.2.perfmeasures_reportingsection")

# coding stage
rs2 <- c("6.2.software_reportingsection")

# analysis stage
rs3 <- c("9.2.analysis_exploratory_reporting",
        "10.2.analysis_perfmeasures_reportingsection",
        "10.1.analysis_perfmeasures_reporting",
        "11.2.analysis_montecarlo_reportingsection")

```



### Literature search results

```{r}
# make year as factor
journal_count <- as.data.frame(table(survey$journal))
journal_count <- journal_count %>% 
  select(Journal="Var1",Freq="Freq") %>%
  arrange(desc(Freq))
kable(journal_count)

# make year as factor
survey$paper_year <-  as.factor(survey$paper_year)

# plot histogram
ggplot(survey, aes(x = paper_year)) +
  geom_bar(aes(fill = paper_year)) +
  xlab("") +
  ylab("Count") +
  ggtitle("Publication year") +
  scale_fill_manual(values=rep("lightblue",6)) +
  scale_y_continuous(breaks = seq(0, 28, by = 5), limits = c(0, 28)) +
  theme(
    axis.text.x = element_text(angle = 40, hjust=1, size = 12), 
    axis.text.y = element_text(size = 12),  
    axis.title.x = element_text(size = 12), 
    axis.title.y = element_text(size = 12), 
    panel.background = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(color = "grey50"),
    panel.grid.minor.y = element_line(color = "grey50")
  ) + theme(legend.position="none")

```


## Reporting results

The following questions were focused on simulation studies reporting quality:

`1.1.aims_reporting` Are the aims of the simulation defined?

`2.1.datagen_reporting` Is the data-generating mechanisms described? 

`2.4.datagen_informed`	Are the specifications of the data-generating mechanisms informed (i.e. justified with a reference or narrative justification)?

`3.1.estimandtarget_reporting`	Were the estimand(s), statistical target and task of the simulation study described?

`4.1.method_reporting`	Is the study citing other relevant statistical methodology literature on the topic?

`5.1.perfmeasures_reporting` Were all performance measures planned to be investigated defined or described?

`5.3.perfmeasures_lessknown`	If a performance measure planned to be used is less-known, was the formula given explicitly?

`6.1.software_reporting`	Is/Are all software(s) and all package(s) used in the simulation study reported explicitly?

`6.4.software_Rpackage` If R statistical software was used, were the packages that were used reported?

`7.1.code_datagen`	Is the code to execute the simulation study made available?

`7.2.code_datagen_allsteps`	Does the code include data generating and analysis simulation steps?

`8.1.code_perfmeasures`	Is the code to obtain performance measures made available?


```{r}

# Function to make horizontal histogram of reporting practices questions
create_plot <- function(survey, r_vector, title) {
  r_long <- survey %>% 
    select(all_of(r_vector)) %>%
    pivot_longer(cols = everything())

  count_r <- r_long %>% 
    group_by(name, value) %>% 
    count()

  count_r <- count_r %>% 
    group_by(name) %>% 
    mutate(total=sum(n))

  percent_r <- count_r %>% 
    mutate(proportion = n/total,
           percentage = proportion * 100)

  percent_r$name <- factor(percent_r$name, levels=rev(r_vector))
  percent_r$value <- replace_na(percent_r$value, "N.A.")
  percent_r$value <- factor(percent_r$value, levels=c("N.A.", "No", "Yes"))

  
  ggplot(data = percent_r, aes(x = name, y = percentage, fill = value)) +
    geom_col(width = 0.7, color = "black") +
    coord_flip(ylim = c(0, 100)) +
    scale_fill_manual(values = c("N.A."="white", "No"="#713979", "Yes"="#4DAF4A")) +
    theme(legend.position = "bottom", 
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank()) +
    ggtitle(title) +
    theme(axis.text.x = element_text(size=14))  +
    theme(axis.text.y = element_text(size=14))  +
    ylab("Percentage (%)") +
    xlab("Survey Question")
}

# Apply the function to each question group
fig1 <- create_plot(survey, r1, "Reporting in planning stage")
fig2 <- create_plot(survey, r2, "Reporting in coding stage")
fig3 <- create_plot(survey, r3, "Reporting in analysis stage")
supp1 <- create_plot(survey, r, "Reporting practices")

# Plot the figures
print(supp1)

```

## Reporting section results

The following questions were focused on simulation studies reporting section:

`1.2.aims_reportingsection`	Are the aims of the simulation study defined in the Introduction or Methods section?

`2.2.datagen_reportingsection`	Is the data-generating mechanisms described in the Methods section?

`2.5.2.datagen_parameters_section`	Are the varying parameters reported explicitly in the Methods section?

`2.6.2.datagen_conditions_section`	Are the number of conditions reported explicitly in the Methods section?

`2.7.2.datagen_simulationruns_section`	Are the number simulation runs reported explicitly in the Methods section?

`3.2.estimandtarget_reportingsection`	Is/Are the estimands or statistical targets/task described in the Methods section?

`4.2.method_reportingsection`	Is the study citing other relevant statistical methodology in the Introduction and/or Methods?

`5.2.perfmeasures_reportingsection`	Were performance measures planned to be investigated defined or described in the Methods section?

`6.2.software_reportingsection`	Were the software(s) and package(s) used in the simulation study reported in Methods or Results section?

`9.2.analysis_exploratory_reporting` Was a working example or case study reported in the Results or Supplementary Material section?

`10.2.analysis_perfmeasures_reportingsection`	Were all performance measures results reported in the Results or Supplementary Material? 

`11.2.analysis_montecarlo_reportingsection`	Are Monte Carlo error reported in the Results or Supplementary Material section?


```{r}

# Apply the function to each question group
fig4 <- create_plot(survey, rs1, "Reporting section for planning stage")
fig5 <- create_plot(survey, rs2, "Reporting section for coding stage")
fig6 <- create_plot(survey, rs3, "Reporting section for analysis stage")
supp2 <- create_plot(survey, rs, "Reporting section practices")

# Plot the figures
print(supp2)

# Make plot for main text
Figure_ReportingPractices <- supp1 / supp2 +
  plot_annotation(tag_levels = 'A')

# save 
ggsave("output/Figure_ReportingPractices_allStages.png", Figure_ReportingPractices)

```



## Characteristics results

The following questions were focused on simulation studies characteristics:

**Planning stage**

`1.3.aims_purpose`	What is the purpose of the simulation study?

`2.3.datagen_type`	 What type of data-generating mechanism was used?

`2.4.datagen_informed` Are the specifications of the data-generating mechanisms informed?

`2.5.1.datagen_parameters` How many varying parameters were used in data-generating mechanisms?

`2.6.1.datagen_conditions`	How many conditions were used in total in the simulation study? (varying parameter combination)

`2.7.1.datagen_simulationruns`	What is the number of simulation runs performed? (report the maximum number used)

`2.7.3.datagen_simulationruns_justification` Was the number simulation runs justified?

`2.8.datagen_distribution`	If a parametric model was used, what was/were the distribution type(s) used?  

`3.3.estimandtarget_type`	What is/are the estimands or statistical targets/task of the simulation study?

`4.3.method_type`	What is the simulation study evaluating? 

`5.4.perfmeasures`	What performance measure(s) were planned to be investigated in the simulation study? 



```{r}
###### 1: Aims --------------------------------------------------------------------------------

#table(survey$`1.3.aims_purpose`, useNA = "ifany")

# Save aims response as separate vector
aims_purpose <- survey$`1.3.aims_purpose`

#  Format the vector into a dataframe
aims_dat <- data.frame(AimsPurpose = aims_purpose) %>%
  mutate(category = case_when(
    AimsPurpose=="Evaluating analytical method" ~ "Analytical\n method",
    AimsPurpose=="Designing and/or planning of study" ~ "Study\n design",
    AimsPurpose=="Both"|AimsPurpose=="Designing and/or planning of study;Evaluating analytical method" ~ "Both"
    )) %>%
  group_by(category) %>%
  summarise(counts = n()) %>%
  mutate(percentage = counts / sum(counts) * 100)

# Plot histogram with ordered categories
plot_aims <- ggplot(aims_dat, aes(x = reorder(category, -counts), y = counts, fill = category)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 80, by = 10)) +
  #coord_flip() + 
  labs(title = "Aims and Purposes", x = "", y = "percentage") +
  theme(axis.text.x = element_text(size=14))  +
  theme(axis.text.y = element_text(size=14)) +
  theme(legend.position = "none") +
  coord_fixed(ratio=0.04)
plot_aims


#table(survey$`1.3.aims_purpose`, useNA = "ifany")


```

```{r}
###### 2: Data-generating mechanism ----------------------------------------------------------------

####### Data-generating Type #######

#table(survey$`2.3.datagen_type`, useNA = "ifany")

# save data-generation type as separate vector
datgen_type <- survey$`2.3.datagen_type`

# Format the vector into a dataframe and clean up the categories
datgen_type <- data.frame(Type=datgen_type) %>%
  mutate(Type = case_when(
           Type == "Parametric model (\"known\");Resampling method (\"unknown\")" ~ "Both",
           grepl("Parametric model", Type, ignore.case = T) ~ "Parametric",
           grepl("Resampling method", Type, ignore.case = T) ~ "Resampling",
           TRUE ~ "NA"
         ))

# Derive the counts and percentages
datgen_type_summary <- datgen_type %>%
  group_by(Type) %>%
  summarise(counts = n()) %>%
  mutate(Percentage = counts / sum(counts) * 100) %>% 
  arrange(desc(counts)) %>%
  mutate(Type = factor(Type, levels = unique(Type))) 


# Plot histogram of data-generation type
plot_datgen <- ggplot(datgen_type_summary, aes(x = reorder(Type, -counts), y = counts, fill = Type)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  #coord_flip() +
  labs(title = "Data-generating type", x = "", y = "percentage") +
  scale_y_continuous(breaks = seq(0, 80, by = 10)) +
  theme(axis.text.x = element_text(size=14, angle=45, hjust=1))  +
  theme(axis.text.y = element_text(size=14))  +
  theme(legend.position = "none") +
  coord_fixed(ratio=0.04)
plot_datgen


####### Data-generating parameters justified #######

#table(survey$`2.4.datagen_informed`, useNA = "ifany")
#
# save data-generation type as separate vector
datgen_inform <- survey$`2.4.datagen_informed`
datgen_inform <- data.frame(datgen_inform)

# Derive the counts and percentages
datgen_inform_summary <- datgen_inform %>%
  group_by(datgen_inform) %>%
  summarise(counts = n()) %>%
  mutate(Percentage = counts / sum(counts) * 100) %>% 
  arrange(desc(counts)) %>%
  mutate(datgen_inform = factor(datgen_inform, levels = unique(datgen_inform))) 


# Plot histogram of data-generation type
ggplot(datgen_inform_summary, aes(x = reorder(datgen_inform, counts), y = counts, fill = datgen_inform)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Data-generating justification", x = "", y = "counts")  +
  theme(axis.text.x = element_text(size=14))  +
  theme(axis.text.y = element_text(size=14))


####### Parameters, conditions and runs #######

##### Number of parameters
par_mean <- mean(survey$`2.5.1.datagen_parameters`, na.rm=T)
par_min <- min(survey$`2.5.1.datagen_parameters`, na.rm=T)
par_max <- max(survey$`2.5.1.datagen_parameters`, na.rm=T)

# Set up data to plot
datgen_pars <- data.frame(pars=survey$`2.5.1.datagen_parameters`)
datgen_pars_clean <- na.omit(datgen_pars)
# histogram of number of parameters
plot_pars <- ggplot(datgen_pars_clean, aes(x=pars)) +
  geom_histogram(binwidth = 1, colour = "black", fill = "skyblue") +
  scale_fill_brewer(palette = "Pastel2") +
  scale_x_continuous(breaks = seq(0, 14, by = 1)) +
  scale_y_continuous(breaks = seq(0, 28, by=2)) +
  theme_minimal() +
  theme(axis.text.x = element_text(size=14)) +
  theme(axis.text.y = element_text(size=14)) +
  labs(title = "Number of simulated parameters", x = "No. parameters", y = "frequency")
plot_pars



###### Number of conditions
cond_mean <- mean(survey$`2.6.1.datagen_conditions`, na.rm=T)
cond_min <- min(survey$`2.6.1.datagen_conditions`, na.rm=T)
cond_max <- max(survey$`2.6.1.datagen_conditions`, na.rm=T)

cond_med <- median(survey$`2.6.1.datagen_conditions`, na.rm=T)

# Set up data to plot
datgen_cond <- data.frame(cond=survey$`2.6.1.datagen_conditions`)
datgen_cond_clean <- na.omit(datgen_cond)
# make conditions as factor
datgen_cond_clean$cond <- as.factor(datgen_cond_clean$cond)
# histogram number of conditions
plot_conds <- ggplot(datgen_cond_clean, aes(x=cond)) +
  geom_bar(colour = "black", fill = "skyblue") +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 6, by = 1)) +
  #coord_flip() +
  theme(axis.text.x = element_text(size=11, angle=45, hjust=1)) +
  theme(axis.text.y = element_text(size=12)) +
  labs(title = "Number of simulation conditions", x = "No. conditions", y = "frequency")
plot_conds



###### Number of simulation runs
survey$`2.7.1.datagen_simulationruns` <- as.numeric(survey$`2.7.1.datagen_simulationruns`)
runs_mean <- mean(survey$`2.7.1.datagen_simulationruns`, na.rm=T)
runs_min <- min(survey$`2.7.1.datagen_simulationruns`, na.rm=T)
runs_max <- max(survey$`2.7.1.datagen_simulationruns`, na.rm=T)

runs_med <- median(survey$`2.7.1.datagen_simulationruns`, na.rm=T)


# Set up data to plot
datgen_runs <- data.frame(runs=survey$`2.7.1.datagen_simulationruns`)
datgen_runs$runs <- as.factor(datgen_runs$runs)
# histogram of number of simulation runs
plot_simruns <- ggplot(datgen_runs, aes(x=runs)) +
  geom_bar(colour = "black", fill = "skyblue") +
  scale_y_continuous(breaks = seq(0,28, by=5)) +
  theme_minimal() +
  coord_flip() +
  theme(axis.text.x = element_text(size=14)) +
  theme(axis.text.y = element_text(size=14)) +
  labs(title = "Number of simulation runs", x = "No. runs", y = "frequency")
plot_simruns



####### Simulation runs justified #######

#table(survey$`2.7.3.datagen_simulationruns_justification`, useNA = "ifany")

```




```{r}

####### Distribution #######

#table(survey$`2.8.datagen_distribution`)
dist_vector <- survey$`2.8.datagen_distribution`

# Format the vector into a dataframe and clean up the categories
dist_new <- data.frame(Distribution=dist_vector) %>%
  separate_rows(Distribution, sep = ";|,") %>%
  mutate(Distribution = trimws(Distribution),
         Distribution = case_when(
           grepl("normal", Distribution, ignore.case = T) ~ "Normal",
           grepl("binomial", Distribution, ignore.case = T) ~ "Binomial",
           grepl("poisson", Distribution, ignore.case = T) ~ "Poisson",
           grepl("negative binomial", Distribution, ignore.case = T) ~ "Negative Binomial",
           grepl("uniform", Distribution, ignore.case = T) ~ "Uniform",
           grepl("exponential", Distribution, ignore.case = T) ~ "Exponential",
           grepl("gamma", Distribution, ignore.case = T) ~ "Gamma",
           grepl("dirichlet", Distribution, ignore.case = T) ~ "Dirichlet",
           grepl("No parametric distribution used", Distribution, ignore.case = T) ~ "Non parametric",
           TRUE ~ "Other"
         ))

# Derive the counts and percentages
dist_summary <- dist_new %>%
  group_by(Distribution) %>%
  summarise(counts = n()) %>%
  mutate(Percentage = counts / sum(counts) * 100) %>% 
  arrange(desc(counts)) %>%
  mutate(Distribution = factor(Distribution, levels = unique(Distribution))) 


# Plot histogram
plot_dist <- ggplot(dist_summary, aes(x = Distribution, y = counts, fill = Distribution)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0,50, by=5)) +
  labs(title = "Statistical Distributions", x = "", y = "percentage") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=14)) +
  theme(axis.text.y = element_text(size=14)) +
  theme(legend.position = "none")

plot_dist


```



```{r}

###### 3: Estimand/Target ----------------------------------------------------------

# Estimand/target were counted for each time it was mentioned across all studies 

#table(survey$`3.3.estimandtarget_type`)

# Split the entries and unlist into a single vector 
estimand_list <- unlist(strsplit(survey$`3.3.estimandtarget_type`, ";"))
estimand_counts <- table(estimand_list)

# order the counts in decreasing order
estimand_counts <- estimand_counts[order(estimand_counts, decreasing = TRUE)]
estimand_counts <- as.data.frame(estimand_counts)

# rename categories of estimand_list variable
estimand_counts <- estimand_counts %>%
  mutate(estimand_list = case_when(
    estimand_list=="Estimand (estimation)" ~ "Estimation",
    estimand_list=="Null hypothesis (hypothesis testing)" ~ "Hypothesis \ntesting",
    estimand_list=="Selected design (study design)" ~ "Study design",
    estimand_list=="Predictions (prediction)" ~ "Prediction",
    estimand_list=="Model (model selection)" ~ "Model selection"
  ))

# Plot histogram
plot_estarget <- ggplot(estimand_counts, aes(x = reorder(estimand_list, -Freq), y = Freq, fill = estimand_list)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal() +
  #coord_flip() +
  labs(title = "Statistical target", x = "", y = "percentage") +
  scale_y_continuous(breaks = seq(0,70, by=10)) +
  theme(axis.text.x = element_text(size=14, angle=45, hjust=1)) +
  theme(axis.text.y = element_text(size=14)) +
  theme(legend.position = "none")
plot_estarget


#estimand_counts

###### 4: Method -------------------------------------------------------------------

#table(survey$`4.3.method_type`)

# Save counts per category
method_type <- table(survey$`4.3.method_type`)

# order the counts in decreasing order
method_type <- method_type[order(method_type, decreasing = TRUE)]
method_type <- as.data.frame(method_type)

# rename Var1 categories
method_type <- method_type %>%
  mutate(Var1 = case_when(
    Var1=="Existing method(s)" ~ "Existing",
    Var1=="New method or procedure" ~ "New method",
    Var1=="Both (e.g. for comparison purposes)" ~ "Both",
    Var1=="Study design" ~ "Study design"
  ))


# Plot histogram
plot_method <-ggplot(method_type, aes(x = reorder(Var1, -Freq), y = Freq, fill = Var1)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  #coord_flip() +
  labs(title = "Method type", x = "", y = "percentage") +
  theme(axis.text.x = element_text(size=14, angle=45, hjust=1)) +
  theme(axis.text.y = element_text(size=14)) +
  scale_y_continuous(breaks = seq(0,70, by=10)) +
  theme(legend.position = "none") +
  coord_fixed(0.08)

plot_method


###### 5: Performance measures -----------------------------------------------------

#table(survey$`5.4.perfmeasures`)
# make new vector of all performance measures
perf_measures <- survey$`5.4.perfmeasures`


# Obtained data frame with all single entried of performance measures
perf_dat <- data.frame(PerfMeasures = perf_measures) %>%
  # Separate entries by ";" or ","
  separate_rows(PerfMeasures, sep = ";|,") %>%
  # Clean up category names into specified categories
  mutate(category = case_when(
    grepl("Bias", PerfMeasures, ignore.case = TRUE) ~ "Bias",
    grepl("Mean squared error|MSE|relative standard error", PerfMeasures, ignore.case = TRUE) ~ "Mean squared \nerror (MSE)",
    grepl("Confidence interval length|width", PerfMeasures, ignore.case = TRUE) ~ "Confidence \ninterval width",
    grepl("Precision|coefficient of variation|Coefficient of variance", PerfMeasures, ignore.case = TRUE) ~ "Precision",
    grepl("Power", PerfMeasures, ignore.case = TRUE) ~ "Power",
    grepl("Type I|Type II", PerfMeasures, ignore.case = TRUE) ~ "Type I or II error",
    grepl("false positive rate|false negative rate|TPR|TNR|FPR|FNR|classification rate|positive predictive value| misclassification|error rate", PerfMeasures, ignore.case = TRUE) ~ "Classification \nrate",
    grepl("Coverage", PerfMeasures, ignore.case = TRUE) ~ "Coverage",
    #grepl("Accuracy", PerfMeasures, ignore.case = TRUE) ~ "Accuracy",
    grepl("Accuracy", PerfMeasures, ignore.case = TRUE)|is.na(PerfMeasures) ~ "Not reported \nor unclear",
    TRUE ~ "Other"
  )) %>%
  # Derive the counts for each category and the overall percentage
  group_by(category) %>%
  summarise(counts = n()) %>%
  mutate(percentage = counts / sum(counts) * 100) %>%
  ungroup() %>%
  # Arrange by Counts descending for plotting
  arrange(desc(counts))  %>%
  mutate(category = factor(category, levels = unique(category))) 


# Plot histogram with ordered categories
plot_perf <- ggplot(perf_dat, aes(x = reorder(category, -counts), y = counts, fill = category)) +
  geom_bar(stat = "identity") +
  #fill=c(rep("skyblue",8),"#d9700f","skyblue")
  theme_minimal() +
  scale_fill_brewer(palette = "Set3") +
  labs(title = "Performance Measure", x = "", y = "percentage") +
  #coord_flip() +
  scale_y_continuous(breaks = seq(0,60, by=10)) +
  theme(axis.text.x = element_text(size=14, angle = 45, vjust = 1, hjust=1)) +
  theme(axis.text.y = element_text(size=14)) +
  theme(legend.position = "none")
  
plot_perf 

```


```{r}
####################################
##### Save plots for main text #####
####################################


###### Figure 3: overall characteristics 
Figure_Characteristics <- (plot_aims | plot_datgen) /
  (plot_dist | plot_estarget) /
  (plot_method | plot_perf) +
  plot_annotation(tag_levels = 'A') 

# save 
ggsave("output/Figure_Characteristics.png", Figure_Characteristics,
       width = 11, height = 14, units = "in", dpi = 400)


####### Figure 4: data-generating mechanisms summary 
Figure_DataGenMech <- (plot_pars | plot_simruns) / plot_conds +
  plot_annotation(tag_levels = 'A') 

# save 
ggsave("output/Figure_DataGenerating.png", Figure_DataGenMech ,
       width = 10, height = 10, units = "in", dpi = 400)

```






**Coding stage**

`6.3.software_type`	  What software(s) was used to run the simulation study? 

`6.5.software_Rpackage_name`	  If an R package was reported, detail which one(s)

`7.3.code_datagen_basedonprevious`	  Was the code of the data generating mechanism inspired or expanded from another simulation study code?


```{r}
###### 6: Software type
#table(survey$`6.3.software_type`)

# Software list
software_list <- survey$`6.3.software_type`

# Format the vector into a dataframe and clean up the categories
software_dat <- data.frame(Software=software_list) %>%
  #separate_rows(Software, sep = ";|,") %>%
  mutate(Software = case_when(
           Software=="R" ~ "R",
           grepl("JAGS|NIMBLE|R;Stan", Software, ignore.case = T) ~ "R with JAGS,\nNIMBLE or STAN",
           grepl("python", Software, ignore.case = T) ~ "Python",
           grepl("None", Software) ~ "Not reported",
           TRUE ~ "Other"
         ))

# Derive the counts and percentages
software_summary <- software_dat %>%
  group_by(Software) %>%
  summarise(counts = n()) %>%
  mutate(Percentage = counts / sum(counts) * 100) %>% 
  arrange(desc(counts)) %>%
  mutate(Software = factor(Software, levels = unique(Software))) 


# Plot histogram
ggplot(software_summary, aes(x = reorder(Software, counts), y = counts, fill = Software)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  coord_flip()+
  scale_y_continuous(breaks = seq(0,75, by=10)) +
  labs(title = "Statistical software", x = "", y = "counts") +
  theme(axis.text.x = element_text(size=14)) +
  theme(axis.text.y = element_text(size=14)) +
  theme(legend.position = "none")




###### 6: Software R package
# super messy.... and not informative?
package_list <- unlist(strsplit(survey$`6.5.software_Rpackage_name`, ";|,"))
package_list <- data.frame(package_list)
kable(table(package_list), format="html")

```

```{r}
###### 7: Code
#survey$`7.3.code_datagen_basedonprevious`

# define colors
colors <- c("NA" = "grey", "No" = "#377eb8", "Yes" = "#4daf4a")


# make variable as factor
survey$`7.3.code_datagen_basedonprevious` <- factor(survey$`7.3.code_datagen_basedonprevious`,
                                                    levels = c("Yes", "No"))
survey$`7.3.code_datagen_basedonprevious` <- addNA(survey$`7.3.code_datagen_basedonprevious`,
                                                   ifany = TRUE)


# Make histogram
ggplot(survey, aes(x = `7.3.code_datagen_basedonprevious`, fill = `7.3.code_datagen_basedonprevious`)) +
  geom_bar(width = 0.7) +
  #coord_flip() + 
  labs(x = "Code based on previous study", y = "Count", fill = "Answer") + # Set or remove axis labels and legend title
  scale_fill_manual(values = colors) +
  scale_y_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100)) +
  theme_minimal() +
  theme(legend.position = "bottom")  


```

**Analysis stage**

`9.1.analysis_exploratory`	  Was a working example or case study conducted?

`11.1.analysis_montecarlo_uncertainty`	  Are Monte Carlo error quantified to assess the simulation uncertainty? (by running the same simulation under different number of random-number seeds)

```{r}
# yes/no questions - make function to create a horizontal histogram for a given variable
create_hist <- function(survey, variable_name) {
  # get variable from data
  data_to_plot <- survey %>%
    select(all_of(variable_name)) %>%
    count(.data[[variable_name]])

  # define colors
  colors <- c("No" = "#377eb8", "Yes" = "#4daf4a")

  # make histogram
  ggplot(data_to_plot, aes(x = n, y = fct_relevel(.data[[variable_name]], "Yes", "No"), fill = .data[[variable_name]])) +
    geom_bar(stat = "identity", position = "dodge", width = 0.7) +
    #coord_flip() +
    xlab("") +
    ylab("Count") +
    ggtitle("") +
    scale_x_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100)) +
    scale_fill_manual(values = colors) +
    labs(y = variable_name, x = "Count") +
    theme_minimal() +
    theme(legend.position = "bottom",
          aspect.ratio = 1/2) 
}

# get plots for each variable
create_hist(survey, "9.1.analysis_exploratory")
create_hist(survey, "11.1.analysis_montecarlo_uncertainty")
```




## Summarised count results 

Summarised counts of all variables with categories in survey. 

```{r}

# function to set up frequency table for each variable
create_frequency_table <- function(data, variable_name) {
  freq_table <- table(data[[variable_name]], useNA = "ifany")
  data.frame(
    Variable = variable_name,
    Category = names(freq_table),
    Count = as.vector(freq_table)
  )
}

# list of all variable names
variable_names <- c(
  "1.3.aims_purpose", "1.4.preregistration", "2.3.datagen_type", "2.4.datagen_informed",
  "2.7.3.datagen_simulationruns_justification", "4.3.method_type",
  "6.3.software_type", 
  "7.3.code_datagen_basedonprevious", "9.1.analysis_exploratory",
  "11.1.analysis_montecarlo_uncertainty", "1.1.aims_reporting",
  "2.1.datagen_reporting", "3.1.estimandtarget_reporting",
  "4.1.method_reporting", "5.1.perfmeasures_reporting",
  "5.3.perfmeasures_lessknown", "6.1.software_reporting",
  "6.4.software_Rpackage", "7.1.code_datagen",
  "7.2.code_datagen_allsteps", "8.1.code_perfmeasures",
  "10.1.analysis_perfmeasures_reporting", "1.2.aims_reportingsection",
  "2.2.datagen_reportingsection", "2.5.2.datagen_parameters_section",
  "2.6.2.datagen_conditions_section", "2.7.2.datagen_simulationruns_section",
  "3.2.estimandtarget_reportingsection", "4.2.method_reportingsection",
  "5.2.perfmeasures_reportingsection", "6.2.software_reportingsection",
  "9.2.analysis_exploratory_reporting", "10.2.analysis_perfmeasures_reportingsection",
  "11.2.analysis_montecarlo_reportingsection"
)

# make table with summary frequencies of all results
combined_table <- lapply(variable_names, create_frequency_table, data = survey) %>% 
  bind_rows()

# formatted table for output
kable_output <- kable(combined_table, format = "html", caption = "Count of simulation studies per category") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

# colorrrrs
colors <- c("#CCCCFF", "#FFE4B5", "#FD9ECC", "#CCFFCC", "#CCFFFF", "#FFE4E1", "#FFCCFF", "#F0E68C", 
            "#E6E6FA", "#DAE315", "#ADD8E6", "#FEEFD5", "#DDA0DD", "#FFCCCC", "#A5F5EE", "#FFFD58",
            "#E0FFFF", "#D8BFD8", "#F5DEB3", "#AAE1EE", "#D29DEE", "#FAFAD2", "#FFF0F5", "#5EDDD2",
            "#FFF5EE", "#A1EE55", "#F0FFFF", "#F59EFA", "#FDF5E6", "#F1D8F0", "#F1C158", "#FFF0F5", 
            "#F1A993",  "#FDD1FF", "#F0FFFF", "#F66EDD","#F0FFF0", "#F5D42D", "#EDFBD7", "#FDF5E6")


# apply colors to rows
for (i in seq_along(variable_names)) {
  rows <- which(combined_table$Variable == variable_names[i])
  if(length(rows) > 0) {
    kable_output <- row_spec(kable_output, rows, background = colors[i %% length(colors) + 1])
  }
}

# print - tada
kable_output

```


# Appendix

### Desirable properties of an estimator

In statistics, an estimator is a function that is used to estimate a population parameter based on a sample of data. There are several desirable properties that an estimator may possess, including:

-   **Unbiasedness:** An estimator is said to be unbiased if its expected value is equal to the true value of the population parameter being estimated. This means that, on average, the estimator will produce estimates that are close to the true value. $E[\hat{θ}]=θ$

-   **Consistency:** An estimator is said to be consistent if the difference between the estimated value and the true value of the population parameter tends to zero as the sample size increases. This means that the estimator will produce more accurate estimates as the amount of data used to estimate the parameter increases. $\hat{θ_n}→θ$ as $n→∞$

-   **Efficiency:** An estimator is said to be efficient if it has a smaller variance than other estimators that are unbiased and consistent. This means that the estimator will produce estimates that are more precise, on average, than other estimators. $Var(\hat{θ})$

Additionally, other properties:

-   **Sufficiency:** An estimator is said to be sufficient if it is a function of all the relevant information in the data. This means that no other estimator can produce estimates that are more accurate, on average, than the sufficient estimator.

-   **Robustness**: An estimator is robust if it remains relatively unbiased and efficient when the underlying assumptions of the model are violated.

# References

Morris, T. P., White, I. R., & Crowther, M. J. (2019). Using simulation studies to evaluate statistical methods. Statistics in Medicine, 38(11), 2074--2102. <https://doi.org/10.1002/sim.8086>



# Session info

```{r Reproducibility-SessionInfo-R-environment, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width='100%', results='asis'}

sessioninfo::session_info()%>%
  details::details(
    summary = 'Current session info',
    open    = F
  )

```
