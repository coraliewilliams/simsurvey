---
title: "Current Practices of Simulation Studies for Statistical Method Evaluation in Ecology and Evolutionary Biology"
author: "Coralie Williams, Malgorzata Lagisz, Kyle Morrison, Lorenzo Ricolfi, Yefeng Yang, David Warton, and Shinichi Nakagawa"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    toc: true
    toc-location: left
    toc-depth: 2
    theme: sandstone
    embed-resources: true
    code-fold: show
    code-tools: true
    number-sections: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| output: false
#| warning: false
#| label: packages
#| code-overflow: wrap
#| code-fold: true

pacman::p_load(
  rmarkdown, readr, dplyr, purrr, splitstackshape, tidyverse, tidyr, ggplot2,
  vroom, details, sessioninfo, devtools, readxl, forcats, stringr, RColorBrewer
)

options(digits = 3, scipen = 5)
```

# Background

Simulation studies hold a central role in evaluating statistical methodologies. Conducting a simulation study is similar to an experiment. In a simulation study the underlying process of generating the data is known, and the statistical performance can be assessed under diverse scenarios. Here, we present the findings from a survey of 96 research articles in ecology and evolutionary biology journals that use at least one simulation study to evaluate statistical methods.

A framework called ADEMP (Aims, Data-generating mechanisms, Methods, Estimand/statistical target, Performance measures) was introduced by Morris et al. (2019, Stat Med, 38, p2074), and provides key steps to plan, code, analyse, and report simulation studies.

Using the ADEMP framework as a basis, we put together a list of reporting items to characterise and assess the reporting practices of simulation studies. Our results provide an overview of the current state of simulation studies in ecology and evolutionary biology.

# Objectives

Using the ADEMP+ framework we will:

-   Survey the characteristics of simulation studies evaluating statistical methodologies in ecology and evolutionary biology.

-   Assess the reporting practices of simulation studies evaluating statistical methodologies in ecology and evolutionary biology.

# Literature search

## Journal list

```{r journalList, warning=F, message=F}


# Load journal lists
jcr_stats <- read_csv("~/Projects/simsurvey/data/JCR/JCR_JournalResults_09_2022_Statistics_and_Probability.csv", skip = 2)
jcr_eco <- read_csv("~/Projects/simsurvey/data/JCR/JCR_JournalResults_09_2022_Ecology.csv", skip = 2)
jcr_evo <- read_csv("~/Projects/simsurvey/data/JCR/JCR_JournalResults_09_2022_Evolutionary_Biology.csv", skip = 2)

# Clean: delete empty rows and comment lines at the end of files
jcr_stats <- jcr_stats[which(!is.na(jcr_stats$ISSN)),]
jcr_eco <- jcr_eco[which(!is.na(jcr_eco$ISSN)),]
jcr_evo <- jcr_evo[which(!is.na(jcr_evo$ISSN)),]


# Check how many journals overlap between each journal group: none with stats journal list, and 18 journals between eco and evo
stats_in_eco <- jcr_stats[which(jcr_stats$`Journal name` %in% jcr_eco$`Journal name`),]
stats_in_evo <- jcr_stats[which(jcr_stats$`Journal name` %in% jcr_evo$`Journal name`),]
eco_in_evo <- jcr_eco$`Journal name`[which(jcr_eco$`Journal name` %in% jcr_evo$`Journal name`)]
evo_in_eco <- jcr_evo$`Journal name`[which(jcr_evo$`Journal name` %in% jcr_eco$`Journal name`)]

# get concatenated list of eco + evo journal lists
ecoevo <- rbind(jcr_eco, jcr_evo)

## change in code below on 20/09/2023, before this code was merging on all columns and only matched 15 journals and missed 3 journals that were overlapping due to different values in "JIF Quartile" column

# get list of matching and unique journals between eco and evo groups
ecoevo_match <- merge(jcr_eco[,-c(5,8)], jcr_evo[,-c(5,8)], by=c("Journal name","JCR Abbreviation", "ISSN", "eISSN", "Total Citations", "2021 JIF", "2021 JCI", "% of OA Gold"))
ecoevo_match$Category <- "Both"
ecoevo_match$`JIF Quartile` <- "NA"
ecoevo_notmatch <- ecoevo[which(!ecoevo$`Journal name` %in% ecoevo_match$`Journal name`),]


# get merged list of journals from eco and evo
ecoevo_merged <- rbind(ecoevo_match, ecoevo_notmatch)

# save results
write_csv(ecoevo_merged, "~/Projects/simsurvey/data/ecoevo_jcr_journal_list.csv", na="")

```

## Search strings

The custom search string was developed on 13/10/2022 on Web of Science

::: {.callout-tip appearance="minimal" icon="false"}
**(TS=((simulation\*) AND ("method") AND ("compar" OR "evaluat" OR "validat" OR "statist") AND (bias OR coverag\* OR power OR convergence OR"standard error" OR "confidence interval" OR "performance measure" OR "mean error" OR "type I error"))) AND WC=("Ecology" OR "Evolutionary Biology"**
:::

## Inclusion and exclusion criteria

-   Include publications between 01-01-2012 and 01-09-2022.
-   Include studies using simulation for the evaluation of statistical methodologies. Simulation studies that are not assessing methods in statistics will be excluded (for example simulation studies to assess ecological theories: <https://doi.org/10.1016/j.ecolmodel.2019.02.007>).
-   Simulation studies that are part of a review, commentary, letter to the editor or tutorial will be excluded (for example: <https://doi.org/10.1111/2041-210X.1380>) Included studies published in one of predefined journals in Table 1.
-   Include if: Full text is available and full text is in English

::: column-margin
Note, exclusion term criteria were added post-hoc.
:::

# Data cleaning

```{r}
# Load text extraction
survey_raw <- read_excel("~/Projects/simsurvey/data/survey/simulationSurvey_extraction_103papers_21092023.xlsx", na=c("", "NA", NA))

# remove excluded papers
survey <- survey_raw %>% filter(is.na(exclude))

# remove questions that are comments
survey <- survey %>% select(!matches("comment"))

# Names of reporting questions -------------------
r <- c("1.1.aims_reporting", 
       "2.1.datagen_reporting", 
       "3.1.estimandtarget_reporting",
       "4.1.method_reporting",
       "5.1.perfmeasures_reporting",
       "6.1.software_reporting",
       "9.1.analysis_exploratory",
       "10.1.analysis_perfmeasures_reporting",
       "11.1.analysis_montecarlo_uncertainty")

# only planning stage 
r1 <- c("1.1.aims_reporting", 
       "2.1.datagen_reporting", 
       "3.1.estimandtarget_reporting",
       "4.1.method_reporting",
       "5.1.perfmeasures_reporting")

# only coding stage
r2 <- c("6.1.software_reporting")

# only analysis stage
r3 <- c("9.1.analysis_exploratory",
       "10.1.analysis_perfmeasures_reporting",
       "11.1.analysis_montecarlo_uncertainty")


# names of reporting-section questions ------------
rs <- c("1.2.aims_reportingsection", 
        "2.2.datagen_reportingsection", 
        "3.2.estimandtarget_reportingsection",
        "4.2.method_reportingsection",
        "5.2.perfmeasures_reportingsection",
        "6.2.software_reportingsection",
        "9.2.analysis_exploratory_reporting",
        "10.2.analysis_perfmeasures_reportingsection",
        "10.1.analysis_perfmeasures_reporting",
        "11.2.analysis_montecarlo_reportingsection")

# only planning stage
rs1 <- c("1.2.aims_reportingsection", 
        "2.2.datagen_reportingsection", 
        "3.2.estimandtarget_reportingsection",
        "4.2.method_reportingsection",
        "5.2.perfmeasures_reportingsection")

# coding stage
rs2 <- c("6.2.software_reportingsection")

# analysis stage
rs3 <- c("9.2.analysis_exploratory_reporting",
        "10.2.analysis_perfmeasures_reportingsection",
        "10.1.analysis_perfmeasures_reporting",
        "11.2.analysis_montecarlo_reportingsection")

```

# Analysis

### Reporting

The following questions were focused on simulation studies reporting quality:

`1.1.aims_reporting` Are the aims of the simulation defined?

`2.1.datagen_reporting` Is the data-generating mechanisms described? 

`2.4.datagen_informed`	Are the specifications of the data-generating mechanisms informed (i.e. justified with a reference or narrative justification)?

`3.1.estimandtarget_reporting`	Were the estimand(s), statistical target and task of the simulation study described?

`4.1.method_reporting`	Is the study citing other relevant statistical methodology literature on the topic?

`5.1.perfmeasures_reporting` Were all performance measures planned to be investigated defined or described?

`5.3.perfmeasures_lessknown`	If a performance measure planned to be used is less-known, was the formula given explicitly?

`6.1.software_reporting`	Is/Are all software(s) and all package(s) used in the simulation study reported explicitly?

`6.4.software_Rpackage` If R statistical software was used, were the packages that were used reported?

`7.1.code_datagen`	Is the code to execute the simulation study made available?

`7.2.code_datagen_allsteps`	Does the code include data generating and analysis simulation steps?

`8.1.code_perfmeasures`	Is the code to obtain performance measures made available?


```{r}

# Function to make horizontal histogram of reporting practices questions
create_plot <- function(survey, r_vector, title) {
  r_long <- survey %>% 
    select(all_of(r_vector)) %>%
    pivot_longer(cols = everything())

  count_r <- r_long %>% 
    group_by(name, value) %>% 
    count()

  count_r <- count_r %>% 
    group_by(name) %>% 
    mutate(total=sum(n))

  percent_r <- count_r %>% 
    mutate(proportion = n/total,
           percentage = proportion * 100)

  percent_r$name <- factor(percent_r$name, levels=rev(r_vector))
  percent_r$value <- replace_na(percent_r$value, "N.A.")
  percent_r$value <- factor(percent_r$value, levels=c("N.A.", "No", "Yes"))

  
  ggplot(data = percent_r, aes(x = name, y = percentage, fill = value)) +
    geom_col(width = 0.7, color = "black") +
    coord_flip(ylim = c(0, 100)) +
    scale_fill_manual(values = c("N.A."="#E3E3E3", "No"="#713979", "Yes"="#4DAF4A")) +
    theme(legend.position = "bottom", 
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank()) +
    ggtitle(title) +
    ylab("Percentage (%)") +
    xlab("Survey Question")
}

# Apply the function to each question group
fig1 <- create_plot(survey, r1, "Reporting in planning stage")
fig2 <- create_plot(survey, r2, "Reporting in coding stage")
fig3 <- create_plot(survey, r3, "Reporting in analysis stage")
supp1 <- create_plot(survey, r, "Reporting practices")

# Plot the figures
print(fig1)
print(fig2)
print(fig3)
print(supp1)

```

### Reporting section

The following questions were focused on simulation studies reporting section:

`1.2.aims_reportingsection`	Are the aims of the simulation study defined in the Introduction or Methods section?

`2.2.datagen_reportingsection`	Is the data-generating mechanisms described in the Methods section?

`2.5.2.datagen_parameters_section`	Are the varying parameters reported explicitly in the Methods section?

`2.6.2.datagen_conditions_section`	Are the number of conditions reported explicitly in the Methods section?

`2.7.2.datagen_simulationruns_section`	Are the number simulation runs reported explicitly in the Methods section?

`3.2.estimandtarget_reportingsection`	Is/Are the estimands or statistical targets/task described in the Methods section?

`4.2.method_reportingsection`	Is the study citing other relevant statistical methodology in the Introduction and/or Methods?

`5.2.perfmeasures_reportingsection`	Were performance measures planned to be investigated defined or described in the Methods section?

`6.2.software_reportingsection`	Were the software(s) and package(s) used in the simulation study reported in Methods or Results section?

`9.2.analysis_exploratory_reporting` Was a working example or case study reported in the Results or Supplementary Material section?

`10.2.analysis_perfmeasures_reportingsection`	Were all performance measures results reported in the Results or Supplementary Material? 

`11.2.analysis_montecarlo_reportingsection`	Are Monte Carlo error reported in the Results or Supplementary Material section?


```{r}

# Apply the function to each question group
fig4 <- create_plot(survey, rs1, "Reporting section for planning stage")
fig5 <- create_plot(survey, rs2, "Reporting in coding stage")
fig6 <- create_plot(survey, rs3, "Reporting in analysis stage")
supp2 <- create_plot(survey, rs, "Reporting practices")

# Plot the figures
print(fig4)
print(fig5)
print(fig6)
print(supp2)
```


### Characteristics

The following questions were focused on simulation studies characteristics:

**Analysis of planning stage**

`1.3.aims_purpose`	What is the purpose of the simulation study?

`2.3.datagen_type`	 What type of data-generating mechanism was used?

`2.5.2.datagen_parameters`	How many varying parameters were used in data-generating mechanisms?

`2.6.1.datagen_conditions`	How many conditions were used in total in the simulation study? (varying parameter combination)

`2.7.1.datagen_simulationruns`	What is the number of simulation runs performed? (report the maximum number used)

`2.8.datagen_distribution`	If a parametric model was used, what was/were the distribution type(s) used?  

`3.3.estimandtarget_type`	What is/are the estimands or statistical targets/task of the simulation study?

`4.3.method_type`	What is the simulation study evaluating? 

`5.4.perfmeasures`	What performance measure(s) were planned to be investigated in the simulation study? 



```{r}
###### 1: Aims --------------------------------------------------------------------------------

#table(survey$`1.3.aims_purpose`)

survey <- survey %>%
  mutate(`1.3.aims_purpose` = ifelse(`1.3.aims_purpose` == "Designing and/or planning of study;Evaluating analytical method", "Both", `1.3.aims_purpose`))


# get percentages
aims_sum <- survey %>%
  group_by(`1.3.aims_purpose`) %>%
  summarise(Count = n()) %>%
  mutate(Percentage = (Count / sum(Count)) * 100)

# update labels
aims_sum <- aims_sum %>%
  mutate(Label = paste(`1.3.aims_purpose`, sprintf("(%0.2f%%)", Percentage)))


# re-order levels
survey$`1.3.aims_purpose` <- factor(survey$`1.3.aims_purpose`, levels=c("Evaluating analytical method",
                                           "Designing and/or planning of study",
                                           "Both"))


# plot histogram
ggplot(survey, aes(x = `1.3.aims_purpose`)) +
  geom_bar(aes(fill = `1.3.aims_purpose`)) +
  scale_x_discrete(labels = setNames(aims_sum$Label, aims_sum$`1.3.aims_purpose`)) +
  xlab("") +
  ylab("Count") +
  ggtitle("Aims and Purpose") +
  scale_fill_brewer(palette="Set2") +
  theme(
    axis.text.x = element_text(angle = 40, hjust = 1, size = 12), 
    axis.text.y = element_text(size = 12),  
    axis.title.x = element_text(size = 12), 
    axis.title.y = element_text(size = 12), 
    panel.background = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(color = "grey50"),
    panel.grid.minor.y = element_blank()
  ) +
  guides(fill=FALSE)

```

```{r}
###### 2: Data-generating mechanism ----------------------------------------------------------------

####### Type #######

#table(survey$`2.3.datagen_type`)

# rename level
survey <- survey %>%
  mutate(`2.3.datagen_type` = ifelse(`2.3.datagen_type` == "Parametric model (\"known\");Resampling method (\"unknown\")","Both", `2.3.datagen_type`))


# re-order levels
survey$`2.3.datagen_type` <-factor(survey$`2.3.datagen_type`, levels=c("Parametric model (\"known\")",
                                           "Resampling method (\"unknown\")",
                                           "Both"))

# plot histogram
ggplot(survey, aes(x = `2.3.datagen_type`)) +
  geom_bar(aes(fill = `2.3.datagen_type`)) +
  xlab("") +
  ylab("Count") +
  ggtitle("Data-generating mechanism") +
  scale_fill_brewer(palette="Set3") +
  theme(
    axis.text.x = element_text(angle = 40, hjust=1, size = 12), 
    axis.text.y = element_text(size = 12),  
    axis.title.x = element_text(size = 12), 
    axis.title.y = element_text(size = 12), 
    panel.background = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(color = "grey50"),
    panel.grid.minor.y = element_blank()
  ) +
  guides(fill=FALSE)




####### Distribution #######

#table(survey$`2.8.datagen_distribution`)

dist_vector <- survey$`2.8.datagen_distribution`


##------ Version 1 (no breakdown summarise binomial and normal as one)

# format distribution vector
dist_vector <- gsub("Binomial \\(includes\\/assumes Bernoulli\\)", "Binomial", dist_vector)

# make function
count_distributions <- function(dist_vector) {
  
  # Split the entries and unlist into a single vector 
  dist_list <- unlist(strsplit(dist_vector, ";|,"))
  dist_list <- trimws(dist_list)  # trim whitespace
  
  # counts for each distribution
  counts <- table(tolower(dist_list))
  
  # Adjust counts for 'normal' by excluding 'multivariate normal' and 'truncated normal'
  counts["normal"] <- counts["normal"] - counts["multivariate normal"] - counts["truncated normal"]
  
  # Counts for 'binomial' 
  counts["binomial"] <- counts["binomial"] + counts["uniform/binomial"] + counts["negative binomial"]  + counts["beta binomial"]
  
  # Counts for 'uniform' 
  counts["uniform"] <- counts["uniform"] + counts["unif"]
  
  # Counts for 'exponential' 
  counts["exponential"] <- counts["exponential"] + counts["cubed exponential"]
  
  # Replace counts of distributions not listed with "Other" or "NA"
  known_distributions <- c("normal", "multivariate normal", "truncated normal", "beta", "dirichlet", 
                           "gamma", "poisson", "binomial", "uniform", "exponential", "no parametric distribution used")
  counts <- counts[known_distributions]
  
  # Sum all other distributions into "Other"
  counts["other"] <- sum(counts[!names(counts) %in% known_distributions])
  
  # Count "NA" if present
  counts["na"] <- sum(dist_list == "NA", na.rm = TRUE)
  
  # Return the counts
  return(counts)
}




# Example usage:
dist_vector <- survey$`2.8.datagen_distribution`
#count_distributions(dist_vector)


# Get counts of distributions
distribution_counts <- count_distributions(dist_vector)

# Convert counts to dataframe for ggplot and reorder the factor levels by Counts
df_distribution_counts <- data.frame(
  Distribution = factor(names(distribution_counts), 
                        levels = names(distribution_counts)[order(distribution_counts,decreasing = TRUE)]),
  Counts = as.integer(distribution_counts)
)

                   
# Plot histogram with ordered distributions
ggplot(df_distribution_counts, aes(x = Distribution, y = Counts, fill = Distribution)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal() +
  labs(title = "Statistical Distributions (version grouped)", x = "Distribution", y = "Counts") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))




## ---------- Version 2 (breakdown for each binomial and normal)

# old code
# not sure how to treat "beta binomial" => total normal and total binomial or breakdown??
counts <- c(
    "Multivariate normal" = length(grep("multivariate normal", dist_vector, ignore.case = TRUE)),
    "Truncated normal" = length(grep("truncated", dist_vector, ignore.case = TRUE)),
    "Beta" = length(grep("beta", dist_vector, ignore.case = TRUE)),
    "Dirichlet" = length(grep("dirichl", dist_vector, ignore.case = TRUE)),
    "Gamma" = length(grep("gamma", dist_vector, ignore.case = TRUE)),
    "Poisson" = length(grep("poisson", dist_vector, ignore.case = TRUE)),
    "Binomial" = length(grep("binomial", dist_vector, ignore.case = TRUE)),
    "Uniform" = length(grep("uniform", dist_vector, ignore.case = TRUE)),
    "Exponential" = length(grep("exponential", dist_vector, ignore.case = TRUE)),
    "Other" = length(grep("Other", dist_vector, ignore.case = TRUE, value=T))
  )



# Plot histogram with ordered distributions
ggplot(df_distribution_counts, aes(x = Distribution, y = Counts, fill = Distribution)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal() +
  labs(title = "Statistical Distributions (version with breakdown)", x = "Distribution", y = "Counts") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```


```{r}

###### 3: Estimand/Target ----------------------------------------------------------

# not sure if we should count each time it is mentioned as separate (percentage per Target Type)
# or specify the per article (different categories: both, all three, all four...)


### Version 1 (count of all)

#table(survey$`3.3.estimandtarget_type`)

# create new variable "both"

# Split the entries and unlist into a single vector 
estimand_list <- unlist(strsplit(survey$`3.3.estimandtarget_type`, ";"))



### Version 2 (paper specific )

#table(survey$`3.3.estimandtarget_type`)






###### 4: Method -------------------------------------------------------------------

#table(survey$`4.3.method_type`)





###### 5: Performance measures -----------------------------------------------------

#table(survey$`5.4.perfmeasures`)



```

**Analysis of coding stage**

`6.3.software_type`	  What software(s) was used to run the simulation study? 

`6.5.software_Rpackage_name`	  If an R package was reported, detail which one(s)

`7.3.code_datagen_basedonprevious`	  Was the code of the data generating mechanism inspired or expanded from another simulation study code?


```{r}
###### 6: Software type

#table(survey$`6.3.software_type`)

# Software list
software_list <- unlist(strsplit(survey$`6.3.software_type`, ";"))




###### 6: Software R package




```

```{r}
###### 7: Code




```

**Analysis of analysis stage**

`9.1.analysis_exploratory`	  Was a working example or case study conducted?

`11.1.analysis_montecarlo_uncertainty`	  Are Monte Carlo error quantified to assess the simulation uncertainty? (by running the same simulation under different number of random-number seeds)

```{r}
# yes/no questions - make function to create a horizontal histogram for a given variable
create_hist <- function(survey, variable_name) {
  # get variable from data
  data_to_plot <- survey %>%
    select(all_of(variable_name)) %>%
    count(.data[[variable_name]])

  # define colors
  colors <- c("No" = "#377eb8", "Yes" = "#4daf4a")

  # make histogram
  ggplot(data_to_plot, aes(x = n, y = fct_relevel(.data[[variable_name]], "Yes", "No"), fill = .data[[variable_name]])) +
    geom_bar(stat = "identity", position = "dodge", width = 0.7) +
    coord_flip() +
    xlab("") +
    ylab("Count") +
    ggtitle("") +
    scale_x_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100)) +
    scale_fill_manual(values = colors) +
    labs(y = variable_name, x = "Count") +
    theme_minimal() +
    theme(legend.position = "bottom")
}

# get plots
create_hist(survey, "9.1.analysis_exploratory")
create_hist(survey, "11.1.analysis_montecarlo_uncertainty")
```

# Important points

### Example of MCMC studies

Example of papers including an analaysis using Monte Carlo Markov Chain (MCMC)

### Desirable properties of an estimator

In statistics, an estimator is a function that is used to estimate a population parameter based on a sample of data. There are several desirable properties that an estimator may possess, including:

-   **Unbiasedness:** An estimator is said to be unbiased if its expected value is equal to the true value of the population parameter being estimated. This means that, on average, the estimator will produce estimates that are close to the true value. $E[hat{θ}]=θ$

-   **Consistency:** An estimator is said to be consistent if the difference between the estimated value and the true value of the population parameter tends to zero as the sample size increases. This means that the estimator will produce more accurate estimates as the amount of data used to estimate the parameter increases. $\hat{θ_n}→θ$ as $n→∞$

-   **Efficiency:** An estimator is said to be efficient if it has a smaller variance than other estimators that are unbiased and consistent. This means that the estimator will produce estimates that are more precise, on average, than other estimators. $Var(\hat{θ})$

Additionally, combinations:

-   **Sufficiency:** An estimator is said to be sufficient if it is a function of all the relevant information in the data. This means that no other estimator can produce estimates that are more accurate, on average, than the sufficient estimator.

-   **Robustness**: An estimator is robust if it remains relatively unbiased and efficient when the underlying assumptions of the model are violated.

# References

Morris, T. P., White, I. R., & Crowther, M. J. (2019). Using simulation studies to evaluate statistical methods. Statistics in Medicine, 38(11), 2074--2102. <https://doi.org/10.1002/sim.8086>

# Session info

```{r Reproducibility-SessionInfo-R-environment, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width='100%', results='asis'}

sessioninfo::session_info()%>%
  details::details(
    summary = 'Current session info',
    open    = TRUE
  )

```
