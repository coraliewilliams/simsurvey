---
title: "Current Practices of Simulation Studies for Statistical Method Evaluation in Ecology and Evolutionary Biology"
author: "Coralie Williams, Malgorzata Lagisz, Kyle Morrison, Lorenzo Ricolfi, Yefeng Yang, David Warton, and Shinichi Nakagawa"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    toc: true
    toc-location: left
    toc-depth: 2
    theme: sandstone
    embed-resources: true
    code-fold: show
    code-tools: true
    number-sections: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| output: false
#| warning: false
#| label: packages
#| code-overflow: wrap
#| code-fold: true

pacman::p_load(
  rmarkdown, readr, dplyr, purrr, splitstackshape, tidyverse, tidyr, ggplot2,
  vroom, details, sessioninfo, devtools, readxl, forcats, stringr
)

options(digits = 3, scipen = 5)
```

# Background

Simulation studies hold a central role in evaluating statistical methodologies. Conducting a simulation study is similar to an experiment. In a simulation study the underlying process of generating the data is known, and the statistical performance can be assessed under diverse scenarios. Here, we present the findings from a survey of 96 research articles in ecology and evolutionary biology journals that use at least one simulation study to evaluate statistical methods.

A framework called ADEMP (Aims, Data-generating mechanisms, Methods, Estimand/statistical target, Performance measures) was introduced by Morris et al. (2019, Stat Med, 38, p2074), and provides key steps to plan, code, analyse, and report simulation studies.

Using the ADEMP framework as a basis, we put together a list of reporting items to characterise and assess the reporting practices of simulation studies. Our results provide an overview of the current state of simulation studies in ecology and evolutionary biology.

# Objectives

Using the ADEMP+ framework we will:

-   Survey the characteristics of simulation studies evaluating statistical methodologies in ecology and evolutionary biology.

-   Assess the reporting practices of simulation studies evaluating statistical methodologies in ecology and evolutionary biology.

# Literature search

## Journal list

```{r journalList, warning=F, message=F}


# Load journal lists
jcr_stats <- read_csv("~/Projects/simsurvey/data/JCR/JCR_JournalResults_09_2022_Statistics_and_Probability.csv", skip = 2)
jcr_eco <- read_csv("~/Projects/simsurvey/data/JCR/JCR_JournalResults_09_2022_Ecology.csv", skip = 2)
jcr_evo <- read_csv("~/Projects/simsurvey/data/JCR/JCR_JournalResults_09_2022_Evolutionary_Biology.csv", skip = 2)

# Clean: delete empty rows and comment lines at the end of files
jcr_stats <- jcr_stats[which(!is.na(jcr_stats$ISSN)),]
jcr_eco <- jcr_eco[which(!is.na(jcr_eco$ISSN)),]
jcr_evo <- jcr_evo[which(!is.na(jcr_evo$ISSN)),]


# Check how many journals overlap between each journal group: none with stats journal list, and 18 journals between eco and evo
stats_in_eco <- jcr_stats[which(jcr_stats$`Journal name` %in% jcr_eco$`Journal name`),]
stats_in_evo <- jcr_stats[which(jcr_stats$`Journal name` %in% jcr_evo$`Journal name`),]
eco_in_evo <- jcr_eco$`Journal name`[which(jcr_eco$`Journal name` %in% jcr_evo$`Journal name`)]
evo_in_eco <- jcr_evo$`Journal name`[which(jcr_evo$`Journal name` %in% jcr_eco$`Journal name`)]

# get concatenated list of eco + evo journal lists
ecoevo <- rbind(jcr_eco, jcr_evo)

## change in code below on 20/09/2023, before this code was merging on all columns and only matched 15 journals and missed 3 journals that were overlapping due to different values in "JIF Quartile" column

# get list of matching and unique journals between eco and evo groups
ecoevo_match <- merge(jcr_eco[,-c(5,8)], jcr_evo[,-c(5,8)], by=c("Journal name","JCR Abbreviation", "ISSN", "eISSN", "Total Citations", "2021 JIF", "2021 JCI", "% of OA Gold"))
ecoevo_match$Category <- "Both"
ecoevo_match$`JIF Quartile` <- "NA"
ecoevo_notmatch <- ecoevo[which(!ecoevo$`Journal name` %in% ecoevo_match$`Journal name`),]


# get merged list of journals from eco and evo
ecoevo_merged <- rbind(ecoevo_match, ecoevo_notmatch)

# save results
write_csv(ecoevo_merged, "~/Projects/simsurvey/data/ecoevo_jcr_journal_list.csv", na="")

```

## Search strings

The custom search string was developed on 13/10/2022 on Web of Science

::: {.callout-tip appearance="minimal" icon="false"}
## **(TS=((simulation\*) AND ("method") AND ("compar" OR "evaluat" OR "validat" OR "statist") AND (bias OR coverag\* OR power OR convergence OR"standard error" OR "confidence interval" OR "performance measure" OR "mean error" OR "type I error"))) AND WC=("Ecology" OR "Evolutionary Biology"**
:::

## Inclusion and exclusion criteria

-   Include publications between 01-01-2012 and 01-09-2022.
-   Include studies using simulation for the evaluation of statistical methodologies. Simulation studies that are not assessing methods in statistics will be excluded (for example simulation studies to assess ecological theories: <https://doi.org/10.1016/j.ecolmodel.2019.02.007>).
-   Simulation studies that are part of a review, commentary, letter to the editor or tutorial will be excluded (for example: <https://doi.org/10.1111/2041-210X.1380>) Included studies published in one of predefined journals in Table 1.
-   Include if: Full text is available and full text is in English

::: column-margin
Note, exclusion term criteria were added post-hoc.
:::

# Data cleaning

```{r}
# Load text extraction
survey_raw <- read_excel("~/Projects/simsurvey/data/survey/simulationSurvey_extraction_103papers_21092023.xlsx", na=c("", "NA", NA))

# remove excluded papers
survey <- survey_raw %>% filter(is.na(exclude))

# remove questions that are comments
survey <- survey %>% select(!matches("comment"))

# names of reporting questions
r <- c("1.1.aims_reporting", 
       "2.1.datagen_reporting", 
       "3.1.estimandtarget_reporting",
       "4.1.method_reporting",
       "5.1.perfmeasures_reporting",
       "6.1.software_reporting",
       "9.1.analysis_exploratory",
       "10.1.analysis_perfmeasures_reporting",
       "11.1.analysis_montecarlo_uncertainty")

# names of reporting-section questions 
rs <- c("1.2.aims_reportingsection", 
        "2.2.datagen_reportingsection", 
        "3.2.estimandtarget_reportingsection",
        "4.2.method_reportingsection",
        "5.2.perfmeasures_reportingsection",
        "6.2.software_reportingsection",
        "9.2.analysis_exploratory_reporting",
        "10.2.analysis_perfmeasures_reportingsection",
        "10.1.analysis_perfmeasures_reporting",
        "11.2.analysis_montecarlo_reportingsection")

```

# Analysis

### Reporting

The following questions were focused on simulation studies reporting quality:

1.1.aims_reporting

2.1.datagen_reporting

2.4.datagen_informed

3.1.estimandtarget_reporting

4.1.method_reporting

5.1.perfmeasures_reporting

5.3.perfmeasures_lessknown

6.1.software_reporting

6.4.software_Rpackage

7.1.code_datagen

7.2.code_datagen_allsteps

8.1.code_perfmeasures

10.1.analysis_perfmeasures_reporting

```{r}

# pivot longer all questions of "reporting" type
r_long <- survey %>% 
  select(all_of(r)) %>%
  pivot_longer(cols = everything())

# Counting the occurrences of each category within each question
count_r <- r_long %>% 
  group_by(name, value) %>% 
  count()

# Get total number of occurrences for each question
count_r <- count_r %>% 
  group_by(name) %>% 
  mutate(total=sum(n))

# Calculate proportion and percentage
percent_r <- count_r %>% 
  mutate(proportion = n/total,
         percentage = proportion * 100)

# Format order of questions
percent_r$name <- factor(percent_r$name, levels=rev(r))
percent_r$value <- replace_na(percent_r$value, "missing")
percent_r$value <- factor(percent_r$value, levels=c("missing", "No", "Yes"))


# Creating the plot
fig1 <- ggplot(data = percent_r, aes(x = name, y = percentage, fill = value)) +
  geom_col(width = 0.7, color = "black") +
  coord_flip(ylim = c(0, 100)) +
  scale_fill_manual(values = c("#E3E3E3", "#713979", "#4DAF4A")) +
  theme(legend.position = "bottom", 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank()) +
  ylab("Percentage (%)") +
  xlab("Survey Question")
fig1

```

### Reporting section

The following questions were focused on simulation studies reporting section:

1.2.aims_reportingsection

2.2.datagen_reportingsection

2.5.2.datagen_parameters_section

2.6.2.datagen_conditions_section

2.7.2.datagen_simulationruns_section

3.2.estimandtarget_reportingsection

4.2.method_reportingsection

5.2.perfmeasures_reportingsection

6.2.software_reportingsection

9.2.analysis_exploratory_reporting

10.2.analysis_perfmeasures_reportingsection

11.2.analysis_montecarlo_reportingsection

```{r}

# pivot longer all questions of "reporting section" type
rs_long <- survey %>% 
  select(all_of(rs)) %>%
  pivot_longer(cols = everything())

# Counting the occurrences of each category within each question
count_rs <- rs_long %>% 
  group_by(name, value) %>% 
  count()

# Get total number of occurrences for each question
count_rs <- count_rs %>% 
  group_by(name) %>% 
  mutate(total=sum(n))

# Calculate proportion and percentage
percent_rs <- count_rs %>% 
  mutate(proportion = n/total,
         percentage = proportion * 100)

# Format order of questions
percent_rs$name <- factor(percent_rs$name, levels=rev(rs))
percent_rs$value <- replace_na(percent_rs$value, "missing")
percent_rs$value <- factor(percent_rs$value, levels=c("missing", "No", "Yes"))


# Creating the plot
fig2 <- ggplot(data = percent_rs, aes(x = name, y = percentage, fill = value)) +
  geom_col(width = 0.7, color = "black") +
  coord_flip(ylim = c(0, 100)) +
  scale_fill_manual(values = c("#E3E3E3", "#713979", "#4DAF4A")) +
  theme(legend.position = "bottom", 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank()) +
  ylab("Percentage (%)") +
  xlab("Survey Question")
fig2

```

### Characteristics

The following questions were focused on simulation studies characteristics:

1.3.aims_purpose

2.3.datagen_type

2.5.2.datagen_parameters

2.6.1.datagen_conditions

2.7.1.datagen_simulationruns

2.8.datagen_distribution

3.3.estimandtarget_type

4.3.method_type

5.4.perfmeasures

6.3.software_type

6.5.software_Rpackage_name

7.3.code_datagen_basedonprevious

9.1.analysis_exploratory

11.1.analysis_montecarlo_uncertainty

```{r}
###### 1: Aims
table(survey$`1.3.aims_purpose`)

survey <- survey %>%
  mutate(`1.3.aims_purpose` = ifelse(`1.3.aims_purpose` == "Designing and/or planning of study;Evaluating analytical method", "Both", `1.3.aims_purpose`))


# get percentages
aims_sum <- survey %>%
  group_by(`1.3.aims_purpose`) %>%
  summarise(Count = n()) %>%
  mutate(Percentage = (Count / sum(Count)) * 100)

aims_sum <- aims_sum %>%
  mutate(Label = paste(`1.3.aims_purpose`, sprintf("(%0.2f%%)", Percentage)))


# plot histogram
ggplot(survey, aes(x = `1.3.aims_purpose`)) +
  geom_bar(aes(fill = `1.3.aims_purpose`)) +
  scale_x_discrete(labels = setNames(aims_sum$Label, aims_sum$`1.3.aims_purpose`)) +
  xlab("") +
  ylab("Count") +
  ggtitle("Aims and Purpose") +
  scale_fill_brewer(palette="Set2") +
  theme(
    axis.text.x = element_text(angle = 30, hjust = 1, size = 12), 
    axis.text.y = element_text(size = 12),  
    axis.title.x = element_text(size = 12), 
    axis.title.y = element_text(size = 12), 
    panel.background = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(color = "grey50"),
    panel.grid.minor.y = element_blank()
  ) +
  guides(fill=FALSE)



###### 2: Data-generating mechanism

## Data generating type
table(survey$`2.3.datagen_type`)

survey <- survey %>%
  mutate(`2.3.datagen_type` = ifelse(`2.3.datagen_type` == "Parametric model (\"known\");Resampling method (\"unknown\")",
                                     "Both", `2.3.datagen_type`))


# plot histogram
ggplot(survey, aes(x = `2.3.datagen_type`)) +
  geom_bar(aes(fill = `2.3.datagen_type`)) +
  xlab("") +
  ylab("Count") +
  ggtitle("Data-generating mechanism") +
  scale_fill_brewer(palette="Set2") +
  theme(
    axis.text.x = element_text(angle = 10, hjust=1, size = 9), 
    axis.text.y = element_text(size = 12),  
    axis.title.x = element_text(size = 12), 
    axis.title.y = element_text(size = 12), 
    panel.background = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(color = "grey50"),
    panel.grid.minor.y = element_blank()
  ) +
  guides(fill=FALSE)



## Data generating type

(survey$`2.8.datagen_distribution`)




###### 3: Estimand/Target




###### 4: Method

```

# Important points

### Example of MCMC studies

Example of papers including an analaysis using Monte Carlo Markov Chain (MCMC)

### Desirable properties of an estimator

In statistics, an estimator is a function that is used to estimate a population parameter based on a sample of data. There are several desirable properties that an estimator may possess, including:

-   **Unbiasedness:** An estimator is said to be unbiased if its expected value is equal to the true value of the population parameter being estimated. This means that, on average, the estimator will produce estimates that are close to the true value. $E[hat{θ}]=θ$

-   **Consistency:** An estimator is said to be consistent if the difference between the estimated value and the true value of the population parameter tends to zero as the sample size increases. This means that the estimator will produce more accurate estimates as the amount of data used to estimate the parameter increases. $\hat{θ_n}→θ$ as $n→∞$

-   **Efficiency:** An estimator is said to be efficient if it has a smaller variance than other estimators that are unbiased and consistent. This means that the estimator will produce estimates that are more precise, on average, than other estimators. $Var(\hat{θ})$

Additionally, combinations:

-   **Sufficiency:** An estimator is said to be sufficient if it is a function of all the relevant information in the data. This means that no other estimator can produce estimates that are more accurate, on average, than the sufficient estimator.

-   **Robustness**: An estimator is robust if it remains relatively unbiased and efficient when the underlying assumptions of the model are violated.

# References

Morris, T. P., White, I. R., & Crowther, M. J. (2019). Using simulation studies to evaluate statistical methods. Statistics in Medicine, 38(11), 2074--2102. <https://doi.org/10.1002/sim.8086>

# Session info

```{r Reproducibility-SessionInfo-R-environment, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width='100%', results='asis'}

sessioninfo::session_info()%>%
  details::details(
    summary = 'Current session info',
    open    = TRUE
  )

```
